---
---

@string{aps = {American Physical Society,}}

@inproceedings{sharifianTapas,
 author = {Margerm, Steve and Sharifian, Amirali and and Guha, Apala and Shriraman, Arrvindh and Pokam, Giells},
 title = {TAPAS: Generating Parallel Accelerators from Parallel Programs},
 booktitle = {The 51th Annual IEEE/ACM International Symposium on Microarchitecture},
 series = {MICRO-51},
 year = {2018},
 location = {Fukuako, Japan},
 pages = {49:1--49:14},
 articleno = {49},
 numpages = {14},
 url = {http://dl.acm.org/citation.cfm?id=3195638.3195698},
 acmid = {3195698},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
 pdf={tapasMicro2018.pdf},
 slides={tapas_slides.pdf},
 code={https://github.com/sfu-arch/tapas},
 abstract={High-level-synthesis (HLS) tools generate accelerators from software programs to ease the task of building hardware. Unfortunately, current HLS tools have limited support for concurrency, which impacts the speedup achievable with the generated accelerator.
Current approaches only target fixed static patterns (e.g., pipeline, data-parallel kernels). This constraints the ability of software programmers to express concurrency.
Moreover, the generated accelerator loses a key benefit of parallel hardware, dynamic asynchrony, and the potential to hide long latency and cache misses.

We have developed TAPAS, an HLS toolchain for generating parallel accelerators from programs with dynamic parallelism. TAPAS is built on top of Tapir [22], [39], which embeds fork-join parallelism into the compiler’s
intermediate-representation. TAPAS leverages the compiler IR to identify parallelism and synthesizes the hardware logic. TAPAS provides first-class architecture
support for spawning, coordinating and synchronizing tasks during accelerator execution. We demonstrate TAPAS can generate accelerators for concurrent
programs with heterogeneous, nested and recursive parallelism. Our evaluation on Intel-Altera DE1-SoC and Arria-10 boards demonstrates that TAPAS generated accelerators achieve 20× the power efficiency of an Intel Xeon, while maintaining comparable performance. We also show that TAPAS enables lightweight tasks that can be spawned in '10 cycles and enables accelerators to exploit available fine-grain parallelism. TAPAS is a complete HLS toolchain for synthesizing parallel programs to accelerators and is open-sourced.}
}

@inproceedings{sharifianChain,
 author = {Sharifian, Amirali and Kumar, Snehasish and Guha, Apala and Shriraman, Arrvindh},
 title = {CHAINSAW: Von-neumann Accelerators to Leverage Fused Instruction Chains},
 booktitle = {The 49th Annual IEEE/ACM International Symposium on Microarchitecture},
 series = {MICRO-49},
 year = {2016},
 location = {Taipei, Taiwan},
 pages = {49:1--49:14},
 articleno = {49},
 numpages = {14},
 url = {http://dl.acm.org/citation.cfm?id=3195638.3195698},
 acmid = {3195698},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
 pdf={sharifianChain.pdf},
 slides={chain_slide.pdf},
 code={https://github.com/sfu-arch/chainsaw},
 abstract={A central tenet behind accelerators is to partition a program execution into regions with different behavior (e.g., SIMD, Irregular, Compute-Intensive) and then use behavior-specialized architectures [1] for each region. It is unclear whether the gains in efficiency arise from recognizing that a simpler microarchitecture is sufficient for the acceleratable code region or the actual microarchitecture, or a combination of both. Many proposals [2], [3] seem to choose dataflow-based accelerators which encounters challenges with fabric utilization and static power when the available instruction parallelism is below the peak operation parallelism available [4].

In this paper, we develop, Chainsaw, a Von-Neumann based accelerator and demonstrate that many of the fundamental overheads (e.g., fetch-decode) can be amortized by adopting the appropriate instruction abstraction. The key insight is the notion of chains, which are compiler fused sequences of instructions. chains adapt to different acceleration behaviors by varying the length of the chains and the types of instructions that are fused into a chain. Chains convey the producer-consumer locality between dependent instructions, which the Chainsaw architecture then captures by temporally scheduling such operations on the same execution unit and uses pipeline registers to forward the values between dependent operations. Chainsaw is a generic multi-lane architecture (4-stage pipeline per lane) and does not require any specialized compound function units; it can be reloaded enabling it to accelerate multiple program paths. We have developed a complete LLVM-based compiler prototype and simulation infrastructure and demonstrated that a 8-lane Chainsaw is within 73% of the performance of an ideal dataflow architecture, while reducing the energy consumption by 45% compared to a 4-way OOO processor.}
}


@INPROCEEDINGS{sharifianNetwork,
author={M. Soleimani and A. Sharifian and A. Fanian},
booktitle={2013 21st Iranian Conference on Electrical Engineering (ICEE)},
title={An energy-efficient clustering algorithm for large scale wireless sensor networks},
year={2013},
volume={},
number={},
pages={1-6},
abstract={Wireless sensor networks (WSNs) consist of a large number of sensor nodes with limited energy resources. Collecting and transmitting sensed information in an efficient way is one of the challenges in these networks. The clustering algorithm is a solution to reduce energy consumption. It can be helpful to the scalability and network life time. However, the problem of unbalanced energy dissipation is an important issue in cluster based WSNs. In this paper, a new clustering algorithm, named PDKC, is proposed for wireless sensor networks based on node deployment knowledge. However, in PDKC, sensor node location is modelled by Gaussian probability distribution function instead of using GPSs or any other location-aware devices. In the proposed method, cluster heads are selected based on node deployment information, residual energy, node degree and their distance from the base station. The Simulation results indicate that PDKC algorithm prolongs network lifetime, improves the network coverage and balance energy dissipation in comparison to other works.},
keywords={energy conservation;energy consumption;Gaussian distribution;pattern clustering;sensor placement;telecommunication network management;wireless sensor networks;wireless sensor network;energy efficient clustering algorithm;limited energy resource;energy consumption reduction;network life time;network scalability;unbalanced energy dissipation;large scale WSN;sensor node location modelling;Gaussian probability distribution function;GPS;location aware device;cluster head selection;node deployment information;residual energy;node degree;base station;PDKC algorithm;network coverage;balance energy dissipation;Clustering algorithms;Energy consumption;Wireless sensor networks;Algorithm design and analysis;Delays;Wireless communication;Standards;Wireless sensor networks;clustering;energy efficient;coverage;data aggregation},
doi={10.1109/IranianCEE.2013.6599697},
ISSN={2164-7054},
month={May},}

@inproceedings{kumarPeruse,
 author = {Kumar, Snehasish and Srinivasan, Vijayalakshmi and Sharifian, Amirali and Sumner, Nick and Shriraman, Arrvindh},
 title = {Peruse and Profit: Estimating the Accelerability of Loops},
 booktitle = {Proceedings of the 2016 International Conference on Supercomputing},
 series = {ICS '16},
 year = {2016},
 isbn = {978-1-4503-4361-9},
 location = {Istanbul, Turkey},
 pages = {21:1--21:13},
 articleno = {21},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/2925426.2926269},
 doi = {10.1145/2925426.2926269},
 acmid = {2926269},
 publisher = {ACM},
 address = {New York, NY, USA},
 pdf={peruse.pdf},
 slides={peruse_slides.pdf},
 keywords = {Accelerator, machine learning, static analysis},
 abstract = {There exist a multitude of execution models available today for a developer to target. The choices vary from general purpose processors to fixed-function hardware accelerators with a large number of variations in-between. There is a growing demand to assess the potential benefits of porting or rewriting an application to a target architecture in order to fully exploit the benefits of performance and/or energy efficiency offered by such targets. However, as a first step of this process, it is necessary to determine whether the application has characteristics suitable for acceleration.

In this paper, we present Peruse, a tool to characterize the features of loops in an application and to help the programmer understand the amenability of loops for acceleration. We consider a diverse set of features ranging from loop characteristics (e.g., loop exit points) and operation mixes (e.g., control vs data operations) to wider code region characteristics (e.g., idempotency, vectorizability). Peruse is language, architecture, and input independent and uses the intermediate representation of compilers to do the characterization. Using static analyses makes Peruse scalable and enables analysis of large applications to identify and extract interesting loops suitable for acceleration. We show analysis results for unmodified applications from the SPEC CPU benchmark suite, Polybench, and HPC workloads.

For an end-user it is more desirable to get an estimate of the potential speedup due to acceleration. We use the workload characterization results of Peruse as features and develop a machine-learning based model to predict the potential speedup of a loop when off-loaded to a fixed function hardware accelerator. We use the model to predict the speedup of loops selected by Peruse and achieve an accuracy of 79%.}
}
